{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67100c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import BertTokenizer, BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd2bb10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'corpus_all.txt'  # Replace with the actual path to your text file\n",
    "\n",
    "lines = []  # Empty list to store the lines\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            lines.append(line.strip())  # Append the line to the list, removing any leading/trailing whitespace\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Please check the file path and try again.\")\n",
    "\n",
    "#file_path = 'corpus_all.txt'  # Replace with the actual path to your text file\n",
    "first_words = []  # Empty list to store the first words\n",
    "\n",
    "try:\n",
    "    for line in lines:\n",
    "        words = line.strip().split()  # Split the line into words\n",
    "        if words:\n",
    "            first_word = words[0]  # Get the first word\n",
    "            first_words.append(first_word)  # Append the first word to the list\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Please check the file path and try again.\")\n",
    "\n",
    "\n",
    "# Define the status levels\n",
    "status_levels = [\"happy\",\"angry\",\"fear\",\"disgust\",\"sad\",\"surprise\",]\n",
    "\n",
    "# Initialize a list to store the extracted status levels and sentences\n",
    "extracted_data = []\n",
    "\n",
    "# Iterate over the texts\n",
    "for text in lines:\n",
    "    # Initialize variables to store the extracted status level and sentence\n",
    "    status = \"\"\n",
    "    sentence = \"\"\n",
    "\n",
    "    # Iterate over the status levels\n",
    "    for level in status_levels:\n",
    "        if level in text:\n",
    "            # Split the text based on the status level\n",
    "            split_text = text.split(level, 1)\n",
    "\n",
    "            # Extract the status level and sentence\n",
    "            status = level\n",
    "            sentence = split_text[1].strip()\n",
    "            break\n",
    "\n",
    "    # Append the extracted status level and sentence to the list\n",
    "    extracted_data.append({\"Status\": status, \"Sentence\": sentence})\n",
    "    \n",
    "df = pd.DataFrame(extracted_data)\n",
    "sentences = df['Sentence']\n",
    "labels = df['Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97175710",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TFBertForSequenceClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load BERT tokenizer and model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-multilingual-cased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTFBertForSequenceClassification\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-multilingual-cased\u001b[39m\u001b[38;5;124m'\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(label_mapping))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Tokenize and encode the sentences\u001b[39;00m\n\u001b[0;32m     13\u001b[0m X_train_encoded \u001b[38;5;241m=\u001b[39m tokenizer(X_train, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m'\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TFBertForSequenceClassification' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert labels to numerical values\n",
    "label_mapping = {'happy': 0, 'angry': 1, 'fear': 2, 'disgust': 3, 'sad': 4, 'surprise': 5}\n",
    "numerical_labels = np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentences, numerical_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(label_mapping))\n",
    "\n",
    "# Tokenize and encode the sentences\n",
    "X_train_encoded = tokenizer(X_train, padding=True, truncation=True, return_tensors='tf', max_length=128)\n",
    "X_test_encoded = tokenizer(X_test, padding=True, truncation=True, return_tensors='tf', max_length=128)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_encoded, y_train, epochs=3, batch_size=32, validation_data=(X_test_encoded, y_test))\n",
    "\n",
    "# Make predictions on the data\n",
    "y_pred = model.predict(sequences)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Create the confusion matrix\n",
    "confusion_mtx = confusion_matrix(labels_indexed, y_pred_classes)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mtx)\n",
    "\n",
    "# Generate the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(labels_indexed, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b346fca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
