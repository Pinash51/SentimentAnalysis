{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import re as sub\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    with open('corpus_all.txt', 'r', encoding='utf-8') as file:\n",
    "        lines = []  # Empty list to store the lines\n",
    "        for line in file:\n",
    "            lines.append(line.strip())  # Append the line to the list, removing any leading/trailing whitespace\n",
    "    \n",
    "    with open('stopwords-bn.txt', 'r', encoding='utf-8') as test:\n",
    "        stopwords_bn = test.readlines()\n",
    "        # the above stopwords contains newline \\n\n",
    "        stop_bn = []\n",
    "\n",
    "        for word in stopwords_bn:\n",
    "            stop_bn.append(word.rstrip(\"\\r\\n\"))\n",
    "            \n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Please check the file path and try again.\")\n",
    "\n",
    "#file_path = 'corpus_all.txt'  # Replace with the actual path to your text file\n",
    "first_words = []  # Empty list to store the first words\n",
    "\n",
    "try:\n",
    "    for line in lines:\n",
    "        words = line.strip().split()  # Split the line into words\n",
    "        if words:\n",
    "            first_word = words[0]  # Get the first word\n",
    "            first_words.append(first_word)  # Append the first word to the list\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Please check the file path and try again.\")\n",
    "\n",
    "\n",
    "# Define the status levels\n",
    "status_levels = [\"happy\",\"angry\",\"fear\",\"disgust\",\"sad\",\"surprise\",]\n",
    "\n",
    "# Initialize a list to store the extracted status levels and sentences\n",
    "extracted_data = []\n",
    "\n",
    "# Iterate over the texts\n",
    "for text in lines:\n",
    "    # Initialize variables to store the extracted status level and sentence\n",
    "    status = \"\"\n",
    "    sentence = \"\"\n",
    "\n",
    "    # Iterate over the status levels\n",
    "    for level in status_levels:\n",
    "        if level in text:\n",
    "            # Split the text based on the status level\n",
    "            split_text = text.split(level, 1)\n",
    "\n",
    "            # Extract the status level and sentence\n",
    "            status = level\n",
    "            sentence = split_text[1].strip()\n",
    "            break\n",
    "\n",
    "    # Append the extracted status level and sentence to the list\n",
    "    extracted_data.append({\"Status\": status, \"Sentence\": sentence})\n",
    "    \n",
    "df = pd.DataFrame(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_pre_processing(text):\n",
    "    #remove stop words from the text\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_bn)\n",
    "    #remove punctuation from the text\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    #remove numbers from the text\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    #remove extra spaces from the text\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "df['Sentence'] = df['Sentence'].apply(text_pre_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'বরবর মননয পরধনমনতর গণপরজতনতর বলদশ সরকর মননয পরধনমনতর সবপনর ডজটল বলদশর সবপন বক ললনকর একজন ছতর শত সহসর আশ দশর এক অকততম ভলবস নজক দশর যগয গড তলর এক তবর আকঙকষ ভরত হযছলম নজ হত পরতষঠত দশর অনযতম বশববদযলয যশর বজঞন পরযকত বশববদযলয ডজটল বলদশ গডত ধরণর পরযকত বশববদযলয পরতষঠ নসনদহ দশ পরচলনর বচকষণতর পরচয বহন কর যশর বসর সবপনর পরতষঠন শত শত টক বনযগ দহত উজড কর অতযনত দখর সথ বশববদযলয জবনর সচনলগন অনভব করছ য পওযর আননদ শকষরথদর মনপরন উজড পডলখ কথ বপরত কমলমত শকষরথর সদ দনতপত পরণ নশর হমক শললতহনর গলন ছতরতত বতল হমক নয সমমনত শকষকদর মরত যওয গল কট ফলর হমক পরণ নশর হমক কটকত নযমত ঘটন মননয পরধনমনতর শকষরথর শকষকদর পতমতর মত সমমন একজন চতরথ শরণর করমচরর শকষকদর নযমত ঘটনর শকর ডজটল বলদশর সবপন পক হনদর বহনর মত হন যয চখর ছতর বনর সললতহনর হয পরতবদ বহষকর শকষরথদর ভষ আনদলন মকতযদধর বভষকময গলর কথ সশসনর বলদশ আদরশ বশববদযলয বশববদযলযর পরশসনর পষ সনতরস মদক বযবসয শকষকদর লঞচত কর ছতরদর জবন ছনমন খল ছতরদর লঞচত বরল নজর সথপন সবপনর সনর বল গডর সমপরণ পরপনথ মননয পরধনমনতর শকষক শকষরথর মলত আরতনদ যশরর আকশ বতস পরকমপত বসরর বসর একজন করমচর পরশসনর ছতরছযয অনযয পরতকর আজও ন জবনর সনল সময গল কলস পরকষ দয যশর বশববদযলয বনধ সপতহ যবত কতটক সময পরল সরয দখব শকষক লঞচত হল ছতর লঞচত হল ছতরদর জবন বপনন সহযয পব অনগরহ সবপন গল বচত এগয আসন সদষটর পথ পন গনছ আমর বনত ম নসর উদদন বদল জন পরকশল জব পরযকত বভগ যশর বজঞন পরযকত বশববদযলয'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df['Sentence']\n",
    "# Map sentiment labels to numerical values\n",
    "label_map  = {'happy': 0, 'angry': 1, 'fear': 2, 'disgust': 3, 'sad': 4, 'surprise': 5}\n",
    "y = df['Status'].map(label_map)\n",
    "\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust the number of features as needed\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.010051719934341228\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Assuming you have loaded and preprocessed your data\n",
    "# X_train_tfidf: TF-IDF features, y: class labels\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Oversample the training data using SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train the model\n",
    "kmeans = KMeans(n_clusters=6, n_init=10, random_state=42).fit(X_train_res)\n",
    "\n",
    "# Predict the cluster labels for the test data\n",
    "cluster_labels = kmeans.predict(X_test)\n",
    "\n",
    "# Evaluate clustering quality using silhouette score\n",
    "silhouette_avg = silhouette_score(X_test, cluster_labels)\n",
    "print(\"Silhouette Score:\", silhouette_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: -0.037415658556968975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Preprocess the text data\n",
    "# (Tokenization, stop words removal, stemming, etc.)\n",
    "\n",
    "# Create TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
    "X = vectorizer.fit_transform(df['Sentence']).toarray()  # Convert to dense array\n",
    "\n",
    "# Use Gaussian Mixture Models (GMM) for clustering\n",
    "num_clusters = 6  # Choose the number of clusters\n",
    "gmm = GaussianMixture(n_components=6, random_state=42)\n",
    "gmm.fit(X)\n",
    "\n",
    "# Predict the cluster labels for the data\n",
    "cluster_labels = gmm.predict(X)\n",
    "\n",
    "# Evaluate clustering quality using silhouette score\n",
    "silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "print(\"Silhouette Score:\", silhouette_avg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
